{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration du mod√®le et des hyperparam√®tres\n",
    "\n",
    "Dans la cellule suivante, nous d√©finissons les principaux hyperparam√®tres pour l'entra√Ænement :\n",
    "- Nombre d'√©poques d'entra√Ænement\n",
    "- Taille des batchs\n",
    "- Patience pour l'early stopping\n",
    "- Taux d'apprentissage\n",
    "- Choix du device (GPU/CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©traitement des donn√©es et chargement du dataset\n",
    "\n",
    "- Dans la cellule suivante, nous effectuons plusieurs op√©rations importantes :\n",
    "- Configuration des transformations pour l'augmentation des donn√©es (redimensionnement, rotations, etc.)\n",
    "- Chargement des images et cr√©ation des √©tiquettes\n",
    "- Split des donn√©es en ensembles d'entra√Ænement, validation et test\n",
    "- D√©finition d'une classe Dataset personnalis√©e pour charger les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Pr√©traitement\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),  # Adapt√© pour le CNN (entr√©e 128x128 ‚Üí sortie 16x16 apr√®s 3 poolings)\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset path\n",
    "dataset_path = 'C:\\\\Users\\\\sebas\\\\PycharmProjects\\\\malaria\\\\data\\\\images'\n",
    "parasitized_dir = os.path.join(dataset_path, 'Parasitized')\n",
    "uninfected_dir = os.path.join(dataset_path, 'Uninfected')\n",
    "\n",
    "# Fichiers et √©tiquettes\n",
    "parasitized_files = [os.path.join(parasitized_dir, f) for f in os.listdir(parasitized_dir) if f.endswith('.png')]\n",
    "uninfected_files = [os.path.join(uninfected_dir, f) for f in os.listdir(uninfected_dir) if f.endswith('.png')]\n",
    "parasitized_labels = [0] * len(parasitized_files)\n",
    "uninfected_labels = [1] * len(uninfected_files)\n",
    "\n",
    "all_files = parasitized_files + uninfected_files\n",
    "all_labels = parasitized_labels + uninfected_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split des donn√©es\n",
    " \n",
    "Dans la cellule suivante, nous effectuons la s√©paration des donn√©es en trois ensembles :\n",
    "- Un ensemble de test (20% des donn√©es)\n",
    "- Un ensemble d'entra√Ænement et de validation (80% des donn√©es), qui est ensuite divis√© en :\n",
    "- Un ensemble d'entra√Ænement (64% du total)\n",
    "- Un ensemble de validation (16% du total)\n",
    " \n",
    "Nous utilisons un split stratifi√© pour conserver les proportions de chaque classe.\n",
    "Nous d√©finissons √©galement une classe Dataset personnalis√©e pour charger les images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split stratifi√© test (20%)\n",
    "trainval_files, test_files, trainval_labels, test_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Split stratifi√© val (20% de train_val)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    trainval_files, trainval_labels, test_size=0.2, stratify=trainval_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset personnalis√©\n",
    "class MalariaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, nous d√©finissons l'architecture de notre CNN personnalis√©. Le mod√®le comprend :\n",
    "- 3 couches de convolution avec des filtres de taille 3x3 et un padding de 1\n",
    "- Des couches de pooling pour r√©duire la dimension spatiale\n",
    "- Une couche fully connected avec 128 neurones\n",
    "- Une couche de dropout pour √©viter le surapprentissage \n",
    "- Une couche de sortie avec 2 neurones (classification binaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition d'un mod√®le de r√©seau de neurones pour classer des images (cellule malade ou saine)\n",
    "class CNNMalariaModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):  # On pr√©cise qu'on veut classer en 2 cat√©gories (cellule malade ou saine)\n",
    "        super(CNNMalariaModel, self).__init__()  # Initialisation du mod√®le √† partir de la classe de base nn.Module\n",
    "\n",
    "        # 1√®re couche de convolution : elle regarde des petits morceaux de l'image gr√¢ce au kernel 3x3\n",
    "        # Elle transforme les 3 canaux de couleur (rouge, vert, bleu) en 32 \"cartes de caract√©ristiques\"\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Normalise les valeurs pour aider le r√©seau √† apprendre plus vite\n",
    "\n",
    "        # 2√®me couche : prend les 32 cartes et en cr√©e 64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # 3√®me couche : transforme les 64 cartes en 128 cartes\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # MaxPool : r√©duit la taille des images de moiti√© √† chaque fois (comme un zoom arri√®re)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # GAP (Global Average Pooling) : r√©duit chaque carte √† une seule valeur moyenne\n",
    "        # Cela permet au mod√®le d‚Äôaccepter des images de tailles diff√©rentes\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # 1√®re couche enti√®rement connect√©e : prend les 128 valeurs et en fait 128 nouvelles\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "\n",
    "        # Dropout : coupe certaines connexions au hasard pendant l'entra√Ænement (pour √©viter que le r√©seau ne \"triche\")\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Derni√®re couche : donne 2 valeurs, une pour chaque classe (ex : \"malade\" et \"saine\")\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):  # C‚Äôest ici qu‚Äôon d√©crit comment les donn√©es traversent le r√©seau\n",
    "        # √âtape 1 : premi√®re convolution + normalisation + activation (ReLU = garde les valeurs positives)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # √âtape 2 : deuxi√®me convolution + normalisation + activation + r√©duction de taille\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # √âtape 3 : troisi√®me convolution + normalisation + activation + r√©duction de taille\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # R√©duction √† une seule valeur par carte (gr√¢ce √† GAP)\n",
    "        x = self.gap(x)  # R√©sultat : un petit tableau de forme [batch, 128, 1, 1]\n",
    "\n",
    "        # On \"aplatie\" ce petit tableau en une ligne pour le donner √† la couche suivante\n",
    "        x = x.view(x.size(0), -1)  # Devient [batch, 128]\n",
    "\n",
    "        # Premi√®re couche compl√®tement connect√©e avec ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Application du dropout (pendant l'entra√Ænement uniquement)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Derni√®re couche qui donne 2 scores (un pour chaque classe)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Pas besoin d'ajouter Softmax ici : la fonction de perte CrossEntropy s'en occupe\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, nous cr√©ons les datasets et dataloaders pour l'entra√Ænement, la validation et le test.\n",
    "Nous initialisons √©galement le mod√®le, d√©finissons la fonction de perte (CrossEntropyLoss) et l'optimiseur (Adam).\n",
    "Nous impl√©mentons aussi une fonction d'√©valuation qui calcule la perte et l'exactitude sur un jeu de donn√©es.\n",
    "Enfin, nous mettons en place la boucle d'entra√Ænement avec early stopping pour √©viter le surapprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets & Loaders\n",
    "train_dataset = MalariaDataset(train_files, train_labels, transform=transform_train)\n",
    "val_dataset = MalariaDataset(val_files, val_labels, transform=transform_train)\n",
    "test_dataset = MalariaDataset(test_files, test_labels, transform=transform_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Mod√®le\n",
    "model = CNNMalariaModel().to(device)\n",
    "\n",
    "# Loss et optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'√©valuation\n",
    "Dans la cellule suivante, nous d√©finissons une fonction d'√©valuation `evaluate()` qui permet de calculer la perte et l'exactitude du mod√®le sur un jeu de donn√©es donn√©. Cette fonction sera utilis√©e pour √©valuer les performances du mod√®le sur les ensembles de validation et de test. Elle prend en param√®tres le mod√®le et un dataloader, et retourne la perte moyenne et le pourcentage de pr√©dictions correctes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction d'√©valuation\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables pour l'entra√Ænement\n",
    "- best_acc : stocke la meilleure exactitude obtenue sur l'ensemble de validation\n",
    "- patience_counter : compte le nombre d'√©poques sans am√©lioration pour l'early stopping\n",
    "- best_model_state : sauvegarde l'√©tat du meilleur mod√®le\n",
    "Nous cr√©ons √©galement un dossier 'models' pour sauvegarder les checkpoints du mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entra√Ænement\n",
    "best_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "os.makedirs('models', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entra√Ænement du mod√®le. Pour chaque √©poque :\n",
    "- Nous calculons la perte et l'exactitude sur l'ensemble d'entra√Ænement\n",
    "- Nous √©valuons le mod√®le sur l'ensemble de validation\n",
    "- Nous sauvegardons le meilleur mod√®le si l'exactitude de validation s'am√©liore\n",
    "- Nous appliquons l'early stopping si aucune am√©lioration n'est constat√©e pendant plusieurs √©poques\n",
    "Une barre de progression tqdm affiche l'avancement de l'entra√Ænement avec les m√©triques en temps r√©el."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/552 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 552/552 [04:15<00:00,  2.16it/s, loss=1.0172, acc=82.42%]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
    "    print('-' * 50)\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    print(f'\\nEpoch termin√©: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': best_model_state,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "        }, 'models/best_model_cnn.pth')\n",
    "        print(f'‚úÖ Nouveau meilleur mod√®le sauvegard√© avec une Val Acc de {best_acc:.2f}%')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'Patience: {patience_counter}/{patience}')\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n‚èπÔ∏è Early stopping apr√®s {epoch + 1} epochs sans am√©lioration.')\n",
    "            break\n",
    "\n",
    "# √âvaluation finale\n",
    "print(\"\\nüìä √âvaluation finale sur le test set :\")\n",
    "model.load_state_dict(best_model_state)\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
